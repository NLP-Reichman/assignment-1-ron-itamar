{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FECp14-d_F2e"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za-DgcYB_IQx",
        "outputId": "6fa520fa-31d6-4ec3-939a-566549fe75f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'git' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'mv' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NLP-Reichman/assignment_1.git\n",
        "!mv assignment_1/data data\n",
        "!rm assignment_1/ -r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i2bOXTB8Dvc"
      },
      "source": [
        "# Introduction\n",
        "In this assignment you will be creating tools for learning and testing language models. The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
        "The relevant files are under the data folder:\n",
        "\n",
        "- en.csv (or the equivalent JSON file)\n",
        "- es.csv (or the equivalent JSON file)\n",
        "- fr.csv (or the equivalent JSON file)\n",
        "- in.csv (or the equivalent JSON file)\n",
        "- it.csv (or the equivalent JSON file)\n",
        "- nl.csv (or the equivalent JSON file)\n",
        "- pt.csv (or the equivalent JSON file)\n",
        "- tl.csv (or the equivalent JSON file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "1u1qR7iaq_GU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# from google.colab import files\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHN0tWTurwkN"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i56aKA0K8adr"
      },
      "source": [
        "## Part 1\n",
        "Implement the function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. Our token definition is a single UTF-8 encoded character. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data.\n",
        "\n",
        "Note - do NOT lowercase the sentences in whi HW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "ws_5u7vRrg0o",
        "outputId": "8c4fbc98-1977-4305-c258-bb94db1aadfd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "start_token = '\\uE000'  # Using Private Use Area character\n",
        "end_token = '\\uE001'    # Using Private Use Area character\n",
        "def preprocess() -> list[str]:\n",
        "  '''\n",
        "  Return a list of characters, representing the shared vocabulary of all languages\n",
        "  '''\n",
        "  data_folder_path = \"./data\"\n",
        "\n",
        "  unique_characters = set()\n",
        "  # Iterate through all files in the data folder\n",
        "  for filename in os.listdir(data_folder_path):\n",
        "\n",
        "      # Check if the file is a JSON file\n",
        "      if filename.endswith(\".json\"):\n",
        "          # print(filename)\n",
        "          # Get the full path to the file\n",
        "          file_path = os.path.join(data_folder_path, filename)\n",
        "\n",
        "          # Open the file and read its contents\n",
        "          with open(file_path, \"r\", encoding='utf-8-sig') as f:\n",
        "              # Load the JSON data as a uTF-8 encoded string\n",
        "              json_data = json.load(f)\n",
        "              for key, val in json_data[\"tweet_text\"].items():\n",
        "                #   print(key, val)\n",
        "                  for c in val:\n",
        "                    unique_characters.add(c) # Update the hash with the token in utf-8 encoding\n",
        "\n",
        "                      \n",
        "\n",
        "\n",
        "  # add the 2 special chars to the set:               \n",
        "  unique_characters.add(start_token)\n",
        "  unique_characters.add(end_token)\n",
        "  return sorted(list(unique_characters))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocabulary = preprocess()\n",
        "vocabulary_length = len(vocabulary)\n",
        "# print(f\"Vocabulary length: {vocabulary_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpjtwHW08jyH"
      },
      "source": [
        "## Part 2\n",
        "Implement the function *lm* that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant *n*-1 sequences, and the values are dictionaries with the *n*_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{ \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25}, \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1} }\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "uySEXdEUrkq_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lm(lang: str, n: int, smoothed=False) -> dict[str, dict[str, float]]:\n",
        "    '''\n",
        "    Return a language model for the given lang and n_gram (n)\n",
        "    :param lang: the language of the model\n",
        "    :param n: the n_gram value\n",
        "    :return: a dictionary where the keys are n_grams and the values are dictionaries\n",
        "              with the n_th tokens and their corresponding probabilities to occur\n",
        "    '''\n",
        "    model = {}\n",
        "    data_folder_path = \"./data\"\n",
        "    filename = f\"{lang}.json\"\n",
        "    file_path = os.path.join(data_folder_path, filename)\n",
        "    \n",
        "    # Open the file and read its contents\n",
        "    with open(file_path, \"r\", encoding='utf-8-sig') as f:\n",
        "        json_data = json.loads(f.read())\n",
        "        # Iterate through each sentence in the corpus\n",
        "        for key, sentence in json_data[\"tweet_text\"].items():\n",
        "            # Iterate through each n-gram in the sentence\n",
        "            # add n-1 start tokens and 1 end token to the sentence:\n",
        "            sentence = start_token * (n-1) + sentence + end_token\n",
        "          \n",
        "            for i in range(len(sentence) - n + 1):\n",
        "                ngram = sentence[i:i+n-1]\n",
        "                next_token = sentence[i+n-1]\n",
        "\n",
        "                # Check if the n-gram is not already in the model\n",
        "                if ngram not in model:\n",
        "                    model[ngram] = {}\n",
        "                # handle next_token:\n",
        "                if next_token not in model[ngram]:\n",
        "                    model[ngram][next_token] = 0\n",
        "                model[ngram][next_token] += 1\n",
        "                    \n",
        "    \n",
        "    # Calculate the probabilities for each n-gram\n",
        "    for ngram in model:\n",
        "        if smoothed:\n",
        "            # add all unseen chars from the vocabulary to the model, with a value of 1 (smoothing):\n",
        "            for char in vocabulary:\n",
        "                if char not in model[ngram]:\n",
        "                    model[ngram][char] = 1\n",
        "                else:\n",
        "                    model[ngram][char] += 1      \n",
        "\n",
        "        total_count = sum(model[ngram].values())\n",
        "        for token in model[ngram]:\n",
        "            model[ngram][token] /= total_count\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {},
      "outputs": [],
      "source": [
        "# res = lm(\"en\",2)\n",
        "# print(f'unsmoothed len res: {len(res)}')\n",
        "\n",
        "# res = lm(\"en\",2, smoothed=True)\n",
        "# print(f'smoothed len res: {len(res)}')\n",
        "# print(res['a'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZnk7Ke8rW5"
      },
      "source": [
        "## Part 3\n",
        "Implement the function *eval* that returns the perplexity of a model (dictionary) running over the data file of the given target language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "id": "ef-EglxXrmk2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "def eval(model: dict, target_lang: str) -> float:\n",
        "  '''\n",
        "  Return the perplexity value calculated over applying the model on the text file\n",
        "  of the target_lang language.\n",
        "  :param model: the language model\n",
        "  :param target_lang: the target language\n",
        "  :return: the perplexity value\n",
        "  '''\n",
        "  n = len(list(model.keys())[0]) + 1\n",
        "\n",
        "  data_folder_path = \"./data\"\n",
        "  filename = f\"{target_lang}.json\"\n",
        "  file_path = os.path.join(data_folder_path, filename)\n",
        "  perplexity = 0\n",
        "  # Open the file and read its contents\n",
        "  with open(file_path, \"r\", encoding='utf-8-sig') as f:\n",
        "      json_data = json.loads(f.read())\n",
        "      # Iterate through each sentence in the corpus\n",
        "      perplexities = []\n",
        "      for key, sentence in json_data[\"tweet_text\"].items():\n",
        "        sentence = start_token * (n-1) + sentence + end_token\n",
        "        sentence_enthropy = 0\n",
        "        # probabilities= [] (only to test the calculation)\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngram = sentence[i:i+n-1]\n",
        "            next_token = sentence[i+n-1]\n",
        "            if ngram in model:\n",
        "                if next_token in model[ngram]:\n",
        "                    # probabilities.append(model[ngram][next_token])\n",
        "                    sentence_enthropy +=  ( -1 * np.log2(model[ngram][next_token]))\n",
        "                else:\n",
        "                    # probabilities.append(1/vocabulary_length)\n",
        "                    sentence_enthropy += -1 * np.log2(1/vocabulary_length) # # todo  review\n",
        "                    # throw an exception if the token is not in the model's keys\n",
        "                    # raise Exception(f\"{next_token} NOT IN MODEL for {ngram}\")\n",
        "                    # sentence_enthropy +=  ( -1 * math.log2(model[ngram]['unknown']))\n",
        "            else:\n",
        "                #raise an exception if the ngram is not in the model's keys\n",
        "                # raise Exception(\"NOT IN MODEL\")\n",
        "                # probabilities.append(1/vocabulary_length)\n",
        "                sentence_enthropy += -1 * np.log2(1/vocabulary_length) # # todo  review\n",
        "              \n",
        "        # entropies = [-np.log2(prop) for prop in probabilities]\n",
        "        # entropy = np.average(entropies)\n",
        "        # perplexity = 2 ** entropy\n",
        "        # # assert similarity between perplexity and the perplexity calculated from the sentence's entropy\n",
        "        # # do not use == but use some tolerance, as the calculation of the perplexity is not accurate\n",
        "        # assert abs(perplexity - 2 ** (sentence_enthropy/(len(sentence) - n + 1))) < 0.0001\n",
        "        \n",
        "        perplexities.append(2 ** (sentence_enthropy/(len(sentence) - n + 1)))\n",
        "        # print(f'{sentence}\\nperplexity: {2 ** (sentence_enthropy/(len(sentence) - n + 1))}')\n",
        "        \n",
        "  res =  np.average(perplexities)\n",
        "  \n",
        "#   print(f'perplexity: {res}')\n",
        "  return res\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for n in range(3, 4):\n",
        "#     for eval_lang in [\"en\", \"es\", \"fr\"]:\n",
        "#         model = lm(\"en\", n, smoothed=False)\n",
        "#         perplexity =  eval(model, eval_lang)\n",
        "#         print(f\"Eval lang {eval_lang} on en  SMOOTHED n=={n} : Perplexity: {perplexity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZYVc7hB84LP"
      },
      "source": [
        "## Part 4\n",
        "Implement the *match* function that calls *eval* using a specific value of *n* for every possible language pair among the languages we have data for. You should call *eval* for every language pair four times, with each call assign a different value for *n* (1-4). Each language pair is composed of the source language and the target language. Before you make the call, you need to call the *lm* function to create the language model for the source language. Then you can call *eval* with the language model and the target language. The function should return a pandas DataFrame with the following four columns: *source_lang*, *target_lang*, *n*, *perplexity*. The values for the first two columns are the two-letter language codes. The value for *n* is the *n* you use for generating the specific perplexity values which you should store in the forth column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "id": "16ew9aZWroPC"
      },
      "outputs": [],
      "source": [
        "def match() -> pd.DataFrame:\n",
        "  '''\n",
        "  Return a DataFrame containing one line per every language pair and n_gram.\n",
        "  Each line will contain the perplexity calculated when applying the language model\n",
        "  of the source language on the text of the target language.\n",
        "  :return: a DataFrame containing the perplexity values\n",
        "  '''\n",
        "  perplexity_values = []\n",
        "  for n in range(1, 5):\n",
        "    for model_lang in [\"en\", \"es\", \"fr\", \"in\", \"it\", \"nl\", \"pt\", \"tl\"]:\n",
        "      model = lm(model_lang, n, True)\n",
        "\n",
        "      for lang in [\"en\", \"es\", \"fr\", \"in\", \"it\", \"nl\", \"pt\", \"tl\"]:\n",
        "        perplexity = eval(model, lang)\n",
        "        perplexity_values.append({\"source\": model_lang, \"target\": lang, \"n\": n, \"perplexity\": perplexity})\n",
        "        # print(f'source: {model_lang}, target: {lang}, n: {n}, perplexity: {perplexity}')\n",
        "  return pd.DataFrame(perplexity_values)\n",
        "\n",
        "# df = match()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(df[3]['en'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAQoR0dH9C3T"
      },
      "source": [
        "## Part 5\n",
        "Implement the *generate* function which takes a language code, *n*, the prompt (the starting text), the number of tokens to generate, and *r*, which is the random seed for any randomized action you plan to take in your implementation. The function should start generating tokens, one by one, using the language model of the given source language and *n*. The prompt should be used as a starting point for aligning on the probabilities to be used for generating the next token.\n",
        "\n",
        "Note - The generation of the next token should be from the LM's distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "id": "CpCm24-RrpuA"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[403], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# print(generate(\"en\", 3, \"I am \", 20, 5))\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n",
            "Cell \u001b[1;32mIn[403], line 43\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(lang, n, prompt, number_of_tokens, r)\u001b[0m\n\u001b[0;32m     40\u001b[0m   generated_text \u001b[38;5;241m=\u001b[39m start_token \u001b[38;5;241m*\u001b[39m (n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(generated_text)) \u001b[38;5;241m+\u001b[39m generated_text\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_tokens):\n\u001b[1;32m---> 43\u001b[0m   ngram \u001b[38;5;241m=\u001b[39m \u001b[43mgenerated_text\u001b[49m[\u001b[38;5;241m-\u001b[39mn\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;66;03m# TODO handle the case of shorter prompts\u001b[39;00m\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;66;03m# get next token based on the model and the dustribution in model[ngram] . use the random seed\u001b[39;00m\n\u001b[0;32m     45\u001b[0m   next_token \u001b[38;5;241m=\u001b[39m get_next_token(model, ngram, lang)\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "models_1 = {}\n",
        "# for each language create a model with n=1, no smoothing:\n",
        "for lang in [\"en\", \"es\", \"fr\", \"in\", \"it\", \"nl\", \"pt\", \"tl\"]:\n",
        "  models_1[lang] = lm(lang, 1, False)\n",
        "\n",
        "def get_next_token(model, ngram, lang):\n",
        "  '''\n",
        "  Return the next token based on the given model, ngram, and random value from 0 to 1.\n",
        "  '''\n",
        "  # iterate over the model[ngram] and sum the probabilities until the sum is greater than rand:\n",
        "  sum = 0\n",
        "  # get a random number between 0 to 1:\n",
        "  rand = random.random()\n",
        "  try:\n",
        "    for token, prob in model[ngram].items():\n",
        "      sum += prob\n",
        "      if sum > rand:\n",
        "        return token\n",
        "  except:\n",
        "    # reutrn the most common token in language if ngram is not in the model:\n",
        "    return get_next_token(models_1[lang], '', lang)\n",
        "\n",
        "\n",
        "def generate(lang: str, n: int, prompt: str, number_of_tokens: int, r: int) -> str:\n",
        "  '''\n",
        "  Generate text in the given language using the given parameters.\n",
        "  :param lang: the language of the model\n",
        "  :param n: the n_gram value\n",
        "  :param prompt: the prompt to start the generation\n",
        "  :param number_of_tokens: the number of tokens to generate\n",
        "  :param r: the random seed to use\n",
        "  '''\n",
        "  # initialize random seed with r\n",
        "  random.seed(r)\n",
        "  model = lm(lang, n)\n",
        "  generated_text = prompt\n",
        "  # handle short prompts (pad with start tokens)\n",
        "  if len(generated_text) < n - 1:\n",
        "    generated_text = start_token * (n - 1 - len(generated_text)) + generated_text\n",
        "   \n",
        "  for i in range(number_of_tokens):\n",
        "    ngram = generated_text[-n+1:] # TODO handle the case of shorter prompts\n",
        "    # get next token based on the model and the dustribution in model[ngram] . use the random seed\n",
        "    next_token = get_next_token(model, ngram, lang)\n",
        "    # max(model[ngram], key=model[ngram].get)\n",
        "    generated_text += next_token\n",
        "  return generated_text\n",
        "\n",
        "# print(generate(\"en\", 3, \"I am \", 20, 5))\n",
        "print(generate(\"en\", 3, \"\", 20, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWX8Ugu9INH"
      },
      "source": [
        "## Part 6\n",
        "Play with your generate function, try to generate different texts in different language and various values of *n*. No need to submit anything of that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "metadata": {
        "id": "0ykbMBzG9LWn"
      },
      "outputs": [],
      "source": [
        "# print(generate(\"fr\", 2, \"je suis \", 20, 5))\n",
        "# print(generate(\"fr\", 2, \"je suis \", 20, 5))\n",
        "# print(generate(\"fr\", 2, \"je suis\", 20, 5))\n",
        "\n",
        "# print(generate(\"en\", 2, \"I am \", 20, 5))\n",
        "# print(generate(\"en\", 2, \"I am\", 20, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2jNlDISr9aL"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv48OCT_sIYW"
      },
      "source": [
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JZTlc2ieruqq",
        "outputId": "48177a3b-7f5e-42fd-8ebd-e2f5a6eddf19"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[398], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(res, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Download the results.json file\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[43mfiles\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m####################\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ],
      "source": [
        "####################\n",
        "# PLACE TESTS HERE #\n",
        "# Create tests\n",
        "def test_preprocess():\n",
        "    return {\n",
        "        'vocab_length': len(preprocess()),\n",
        "    }\n",
        "\n",
        "def test_lm():\n",
        "    return {\n",
        "        'english_2_gram_length': len(lm('en', 2)),\n",
        "        'english_3_gram_length': len(lm('en', 3)),\n",
        "        'french_3_gram_length': len(lm('fr', 3)),\n",
        "        'spanish_3_gram_length': len(lm('es', 3)),\n",
        "    }\n",
        "\n",
        "def test_eval():\n",
        "    return {\n",
        "        'english_on_english': round(eval(lm('en', 3), 'en'), 2),\n",
        "        'english_on_french': round(eval(lm('en', 3), 'fr'), 2),\n",
        "        'english_on_spanish': round(eval(lm('en', 3), 'es'), 2),\n",
        "    }\n",
        "\n",
        "def test_match():\n",
        "    df = match()\n",
        "    return {\n",
        "        'df_shape': df.shape,\n",
        "        'en_en_1': df[(df['source'] == 'en') & (df['target'] == 'en') & (df['n'] == 1)]['perplexity'].values[0],\n",
        "        'tl_tl_1': df[(df['source'] == 'tl') & (df['target'] == 'tl') & (df['n'] == 1)]['perplexity'].values[0],\n",
        "        'tl_nl_4': df[(df['source'] == 'tl') & (df['target'] == 'nl') & (df['n'] == 4)]['perplexity'].values[0],\n",
        "    }\n",
        "\n",
        "def test_generate():\n",
        "    return {\n",
        "        'english_2_gram': generate('en', 2, \"I am\", 20, 5),\n",
        "        'english_3_gram': generate('en', 3, \"I am\", 20, 5),\n",
        "        'english_4_gram': generate('en', 4, \"I Love\", 20, 5),\n",
        "        'spanish_2_gram': generate('es', 2, \"Soy\", 20, 5),\n",
        "        'spanish_3_gram': generate('es', 3, \"Soy\", 20, 5),\n",
        "        'french_2_gram': generate('fr', 2, \"Je suis\", 20, 5),\n",
        "        'french_3_gram': generate('fr', 3, \"Je suis\", 20, 5),\n",
        "    }\n",
        "\n",
        "TESTS = [test_preprocess, test_lm, test_eval, test_match, test_generate]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "    try:\n",
        "        cur_res = test()\n",
        "        res.update({test.__name__: cur_res})\n",
        "    except Exception as e:\n",
        "        res.update({test.__name__: repr(e)})\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "files.download('results.json')\n",
        "\n",
        "\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCksAA6RisRQ",
        "outputId": "d0a80d85-f49b-4a1d-bdbc-e9716a9574aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Show the local files, results.json should be there now and\n",
        "# also downloaded to your local machine\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMSfgUtuiux0"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[119], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m temp \u001b[38;5;241m=\u001b[39m lm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\nlp\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# temp = lm(\"en\", 4)\n",
        "# print"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
